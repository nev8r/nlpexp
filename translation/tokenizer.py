from tokenizers import Tokenizer
from tokenizers.models import Unigram
from tokenizers.trainers import UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer
from tokenizers.normalizers import NFKC, Lowercase, Sequence
from tokenizers.processors import TemplateProcessing
from tqdm import tqdm
import os

def train_unigram_tokenizer(corpus_path, vocab_size, output_path, is_chinese=False):
    """
    corpus_path: è¯­æ–™æ–‡ä»¶è·¯å¾„
    vocab_size: è¯æ±‡è¡¨å¤§å°
    output_path: è¾“å‡º json è·¯å¾„
    is_chinese: æ˜¯å¦æ˜¯ä¸­æ–‡è¯­æ–™
    """
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {'ä¸­æ–‡' if is_chinese else 'è‹±æ–‡'} Tokenizer...")

    # åˆå§‹åŒ– tokenizer
    tokenizer = Tokenizer(Unigram())

    # è®¾ç½® normalizerï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
    if is_chinese:
        # ä¸­æ–‡ä¸è½¬å°å†™ï¼Œä½†åšå…¼å®¹å½’ä¸€åŒ–
        tokenizer.normalizer = NFKC()
        # ä¸­æ–‡ä½¿ç”¨ BertPreTokenizerï¼Œå¯ä»¥å¤„ç†ä¸­æ—¥éŸ©å­—ç¬¦åˆ†è¯
        tokenizer.pre_tokenizer = BertPreTokenizer()
    else:
        tokenizer.normalizer = Sequence([NFKC(), Lowercase()])
        tokenizer.pre_tokenizer = Whitespace()

    # è®¾ç½®è®­ç»ƒå™¨
    trainer = UnigramTrainer(
        vocab_size=vocab_size,
        unk_token="[UNK]",
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    )

    # è®­ç»ƒ
    with open(corpus_path, "r", encoding="utf-8") as f:
        tokenizer.train_from_iterator(f, trainer=trainer, length=None)

    # è®¾ç½® post-processorï¼Œä½¿ç¼–ç è¾“å‡ºå¸¦ [CLS] å’Œ [SEP]
    tokenizer.post_processor = TemplateProcessing(
        single="[CLS] $A [SEP]",
        pair="[CLS] $A [SEP] $B [SEP]",
        special_tokens=[
            ("[CLS]", tokenizer.token_to_id("[CLS]")),
            ("[SEP]", tokenizer.token_to_id("[SEP]")),
        ],
    )
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # ä¿å­˜
    tokenizer.save(output_path)
    print(f"âœ… å·²ä¿å­˜è‡³ {output_path}\n")


if __name__ == "__main__":
    train_unigram_tokenizer("./data/train.en", vocab_size=10000, output_path="tokenizers/unigram_tokenizer_en.json", is_chinese=False)
    train_unigram_tokenizer("./data/train.zh", vocab_size=8000, output_path="tokenizers/unigram_tokenizer_zh.json", is_chinese=True)
