nohup: ignoring input
========================================
Start evaluating experiment: lora_rank16_lr8e-5
Model base dir: /root/nlpexp/outputs/lora_rank16_lr8e-5
Results will be saved under: ./eval_results/lora_rank16_lr8e-5
========================================
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged
Result will be saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-110-merged_acc.txt
INFO 10-22 17:12:32 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:12:37 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged'}
INFO 10-22 17:12:53 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:12:53 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:12:53 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:12:54 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:54 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:54 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:12:58.980094194 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:58 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=819583)[0;0m WARNING 10-22 17:12:58 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:59 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged...
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:59 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:12:59 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=819583)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=819583)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:46<00:46, 46.85s/it]
[1;36m(EngineCore_DP0 pid=819583)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 26.94s/it]
[1;36m(EngineCore_DP0 pid=819583)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 29.92s/it]
[1;36m(EngineCore_DP0 pid=819583)[0;0m 
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:13:59 [default_loader.py:268] Loading weights took 59.89 seconds
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:00 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 60.185627 seconds
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:07 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/e47a94f659/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:07 [backends.py:550] Dynamo bytecode transform time: 6.54 s
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:07 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:12 [backends.py:215] Compiling a graph for dynamic shape takes 5.60 s
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:14 [monitor.py:34] torch.compile takes 12.14 s in total
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:14 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:15 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:15 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=819583)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 38.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:01, 36.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:01, 36.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:00<00:01, 38.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 22/67 [00:00<00:01, 40.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 26/67 [00:00<00:01, 37.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:00<00:00, 38.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:00<00:00, 38.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 41/67 [00:01<00:00, 39.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 46/67 [00:01<00:00, 39.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▌  | 51/67 [00:01<00:00, 40.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▎ | 56/67 [00:01<00:00, 40.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:01<00:00, 39.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▊| 66/67 [00:01<00:00, 40.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 38.87it/s]
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:18 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:18 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=819583)[0;0m INFO 10-22 17:14:18 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.16 seconds
INFO 10-22 17:14:19 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:14:19 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  32%|███▏      | 376/1187 [00:00<00:00, 3740.99it/s]Adding requests:  63%|██████▎   | 751/1187 [00:00<00:00, 3738.87it/s]Adding requests:  96%|█████████▌| 1135/1187 [00:00<00:00, 3784.37it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3770.33it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [00:01<38:12,  1.93s/it, est. speed input: 20.69 toks/s, output: 41.39 toks/s]Processed prompts:   0%|          | 2/1187 [00:03<29:34,  1.50s/it, est. speed input: 36.47 toks/s, output: 66.53 toks/s]Processed prompts:   0%|          | 3/1187 [00:05<33:39,  1.71s/it, est. speed input: 40.36 toks/s, output: 80.92 toks/s]Processed prompts:   0%|          | 4/1187 [00:07<37:13,  1.89s/it, est. speed input: 33.12 toks/s, output: 94.94 toks/s]Processed prompts:   0%|          | 5/1187 [00:07<28:14,  1.43s/it, est. speed input: 37.46 toks/s, output: 125.20 toks/s]Processed prompts:   1%|          | 6/1187 [00:08<23:20,  1.19s/it, est. speed input: 48.25 toks/s, output: 152.22 toks/s]Processed prompts:   1%|          | 7/1187 [00:16<1:06:30,  3.38s/it, est. speed input: 28.82 toks/s, output: 113.33 toks/s]Processed prompts:   1%|          | 8/1187 [01:20<7:25:51, 22.69s/it, est. speed input: 6.45 toks/s, output: 48.64 toks/s]  Processed prompts:   3%|▎         | 41/1187 [01:20<27:20,  1.43s/it, est. speed input: 33.73 toks/s, output: 882.76 toks/s]Processed prompts:  14%|█▍        | 164/1187 [01:22<04:13,  4.03it/s, est. speed input: 141.40 toks/s, output: 3926.55 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:25<02:14,  6.91it/s, est. speed input: 207.99 toks/s, output: 6011.40 toks/s]Processed prompts:  22%|██▏       | 262/1187 [01:36<03:25,  4.51it/s, est. speed input: 186.86 toks/s, output: 5391.48 toks/s]Processed prompts:  22%|██▏       | 263/1187 [01:37<03:32,  4.35it/s, est. speed input: 185.72 toks/s, output: 5356.26 toks/s]Processed prompts:  22%|██▏       | 265/1187 [01:49<03:31,  4.35it/s, est. speed input: 168.96 toks/s, output: 4855.03 toks/s]Processed prompts:  22%|██▏       | 266/1187 [02:51<18:51,  1.23s/it, est. speed input: 106.98 toks/s, output: 3073.07 toks/s]Processed prompts:  25%|██▌       | 299/1187 [02:52<11:09,  1.33it/s, est. speed input: 120.15 toks/s, output: 3457.31 toks/s]Processed prompts:  35%|███▌      | 420/1187 [02:53<03:15,  3.92it/s, est. speed input: 165.97 toks/s, output: 4856.06 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:56<01:46,  6.31it/s, est. speed input: 200.76 toks/s, output: 5865.98 toks/s]Processed prompts:  44%|████▎     | 519/1187 [03:02<02:07,  5.22it/s, est. speed input: 195.83 toks/s, output: 5691.52 toks/s]Processed prompts:  44%|████▍     | 523/1187 [03:06<02:27,  4.50it/s, est. speed input: 193.53 toks/s, output: 5584.90 toks/s]Processed prompts:  44%|████▍     | 526/1187 [03:08<02:36,  4.23it/s, est. speed input: 192.38 toks/s, output: 5552.98 toks/s]Processed prompts:  44%|████▍     | 528/1187 [03:19<04:23,  2.50it/s, est. speed input: 182.80 toks/s, output: 5282.66 toks/s]Processed prompts:  45%|████▍     | 529/1187 [03:27<06:23,  1.72it/s, est. speed input: 175.66 toks/s, output: 5074.40 toks/s]Processed prompts:  45%|████▍     | 529/1187 [03:39<06:23,  1.72it/s, est. speed input: 175.66 toks/s, output: 5074.40 toks/s]Processed prompts:  45%|████▍     | 530/1187 [04:20<24:42,  2.26s/it, est. speed input: 139.93 toks/s, output: 4046.02 toks/s]Processed prompts:  47%|████▋     | 563/1187 [04:21<09:31,  1.09it/s, est. speed input: 148.42 toks/s, output: 4293.96 toks/s]Processed prompts:  57%|█████▋    | 679/1187 [04:22<01:59,  4.26it/s, est. speed input: 179.54 toks/s, output: 5183.50 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:24<00:58,  7.16it/s, est. speed input: 203.10 toks/s, output: 5834.12 toks/s]Processed prompts:  65%|██████▌   | 774/1187 [04:29<01:09,  5.98it/s, est. speed input: 201.22 toks/s, output: 5764.22 toks/s]Processed prompts:  66%|██████▌   | 778/1187 [04:30<01:11,  5.72it/s, est. speed input: 201.05 toks/s, output: 5756.17 toks/s]Processed prompts:  66%|██████▌   | 781/1187 [04:34<01:29,  4.56it/s, est. speed input: 198.79 toks/s, output: 5696.63 toks/s]Processed prompts:  66%|██████▌   | 783/1187 [04:35<01:31,  4.42it/s, est. speed input: 198.58 toks/s, output: 5687.87 toks/s]Processed prompts:  66%|██████▌   | 785/1187 [04:35<01:29,  4.49it/s, est. speed input: 198.86 toks/s, output: 5690.25 toks/s]Processed prompts:  66%|██████▋   | 787/1187 [04:36<01:32,  4.34it/s, est. speed input: 199.46 toks/s, output: 5691.16 toks/s]Processed prompts:  66%|██████▋   | 788/1187 [04:46<04:22,  1.52it/s, est. speed input: 192.82 toks/s, output: 5503.08 toks/s]Processed prompts:  66%|██████▋   | 789/1187 [04:54<07:17,  1.10s/it, est. speed input: 187.73 toks/s, output: 5360.63 toks/s]Processed prompts:  67%|██████▋   | 790/1187 [05:45<33:22,  5.05s/it, est. speed input: 160.09 toks/s, output: 4574.33 toks/s]Processed prompts:  69%|██████▉   | 823/1187 [05:45<06:32,  1.08s/it, est. speed input: 168.20 toks/s, output: 4763.43 toks/s]Processed prompts:  79%|███████▉  | 937/1187 [05:46<00:56,  4.40it/s, est. speed input: 189.09 toks/s, output: 5423.46 toks/s]Processed prompts:  86%|████████▋ | 1025/1187 [05:47<00:20,  7.92it/s, est. speed input: 205.52 toks/s, output: 5926.25 toks/s]Processed prompts:  87%|████████▋ | 1032/1187 [05:51<00:22,  6.91it/s, est. speed input: 204.70 toks/s, output: 5903.66 toks/s]Processed prompts:  87%|████████▋ | 1037/1187 [05:51<00:21,  6.93it/s, est. speed input: 205.18 toks/s, output: 5916.04 toks/s]Processed prompts:  88%|████████▊ | 1041/1187 [05:54<00:24,  5.88it/s, est. speed input: 204.38 toks/s, output: 5896.99 toks/s]Processed prompts:  88%|████████▊ | 1044/1187 [05:55<00:24,  5.85it/s, est. speed input: 204.65 toks/s, output: 5904.89 toks/s]Processed prompts:  88%|████████▊ | 1046/1187 [06:00<00:42,  3.29it/s, est. speed input: 201.93 toks/s, output: 5821.01 toks/s]Processed prompts:  88%|████████▊ | 1047/1187 [06:19<00:42,  3.29it/s, est. speed input: 199.35 toks/s, output: 5749.74 toks/s]Processed prompts:  88%|████████▊ | 1048/1187 [06:40<03:37,  1.56s/it, est. speed input: 182.35 toks/s, output: 5258.97 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:40<00:00,  1.56s/it, est. speed input: 206.49 toks/s, output: 5969.48 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:40<00:00,  2.97it/s, est. speed input: 206.49 toks/s, output: 5969.48 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 70%|███████   | 834/1187 [00:00<00:00, 8333.07it/s]100%|██████████| 1187/1187 [00:00<00:00, 8365.04it/s]
Model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-110-merged, Accuracy: 0.43049705139005895
Result saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-110-merged_acc.txt
✅ Evaluation for checkpoint-110 completed successfully
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged
Result will be saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-220-merged_acc.txt
INFO 10-22 17:21:25 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:21:29 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged'}
INFO 10-22 17:21:45 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:21:45 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:21:45 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:21:46 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:47 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:21:51.161989037 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:51 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=834264)[0;0m WARNING 10-22 17:21:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:51 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged...
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:51 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:21:51 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=834264)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=834264)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:47<00:47, 47.10s/it]
[1;36m(EngineCore_DP0 pid=834264)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 26.92s/it]
[1;36m(EngineCore_DP0 pid=834264)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 29.95s/it]
[1;36m(EngineCore_DP0 pid=834264)[0;0m 
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:22:51 [default_loader.py:268] Loading weights took 60.00 seconds
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:22:52 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 60.306043 seconds
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:01 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/fc1f0979e9/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:01 [backends.py:550] Dynamo bytecode transform time: 9.22 s
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:02 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:07 [backends.py:215] Compiling a graph for dynamic shape takes 5.70 s
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:09 [monitor.py:34] torch.compile takes 14.92 s in total
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:10 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:10 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:10 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=834264)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 38.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:01, 38.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:00<00:01, 39.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:00<00:01, 41.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:00<00:01, 41.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:00<00:00, 41.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:00<00:00, 42.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:00<00:00, 42.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 42.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:01<00:00, 42.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:01<00:00, 42.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:01<00:00, 43.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:01<00:00, 41.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 41.31it/s]
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:13 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:13 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=834264)[0;0m INFO 10-22 17:23:13 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.09 seconds
INFO 10-22 17:23:14 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:23:14 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  28%|██▊       | 333/1187 [00:00<00:00, 3329.42it/s]Adding requests:  59%|█████▉    | 698/1187 [00:00<00:00, 3511.34it/s]Adding requests:  91%|█████████ | 1079/1187 [00:00<00:00, 3646.04it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3616.78it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [01:11<23:25:59, 71.13s/it, est. speed input: 0.62 toks/s, output: 28.79 toks/s]Processed prompts:   3%|▎         | 34/1187 [01:11<28:38,  1.49s/it, est. speed input: 32.10 toks/s, output: 972.80 toks/s]Processed prompts:  13%|█▎        | 160/1187 [01:12<04:10,  4.10it/s, est. speed input: 155.95 toks/s, output: 4493.94 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:19<02:28,  6.26it/s, est. speed input: 222.24 toks/s, output: 6587.49 toks/s]Processed prompts:  22%|██▏       | 259/1187 [01:29<02:28,  6.26it/s, est. speed input: 207.06 toks/s, output: 6100.15 toks/s]Processed prompts:  22%|██▏       | 260/1187 [02:39<11:00,  1.40it/s, est. speed input: 111.94 toks/s, output: 3296.42 toks/s]Processed prompts:  22%|██▏       | 261/1187 [02:39<10:54,  1.41it/s, est. speed input: 112.34 toks/s, output: 3306.55 toks/s]Processed prompts:  25%|██▍       | 293/1187 [02:40<07:19,  2.03it/s, est. speed input: 126.26 toks/s, output: 3707.99 toks/s]Processed prompts:  35%|███▌      | 418/1187 [02:41<02:24,  5.34it/s, est. speed input: 177.75 toks/s, output: 5258.29 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:48<01:34,  7.16it/s, est. speed input: 210.19 toks/s, output: 6204.89 toks/s]Processed prompts:  43%|████▎     | 515/1187 [02:59<01:33,  7.16it/s, est. speed input: 202.93 toks/s, output: 5995.95 toks/s]Processed prompts:  43%|████▎     | 516/1187 [03:02<02:28,  4.53it/s, est. speed input: 195.20 toks/s, output: 5757.01 toks/s]Processed prompts:  44%|████▎     | 517/1187 [04:10<08:55,  1.25it/s, est. speed input: 142.91 toks/s, output: 4198.37 toks/s]Processed prompts:  46%|████▋     | 550/1187 [04:11<06:07,  1.73it/s, est. speed input: 150.90 toks/s, output: 4454.61 toks/s]Processed prompts:  57%|█████▋    | 674/1187 [04:12<01:59,  4.30it/s, est. speed input: 185.59 toks/s, output: 5441.57 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:19<01:10,  5.96it/s, est. speed input: 207.57 toks/s, output: 6052.77 toks/s]Processed prompts:  65%|██████▌   | 773/1187 [04:29<01:09,  5.96it/s, est. speed input: 201.32 toks/s, output: 5858.22 toks/s]Processed prompts:  65%|██████▌   | 774/1187 [04:33<01:41,  4.07it/s, est. speed input: 198.20 toks/s, output: 5763.61 toks/s]Processed prompts:  65%|██████▌   | 775/1187 [05:41<05:31,  1.24it/s, est. speed input: 158.88 toks/s, output: 4619.19 toks/s]Processed prompts:  68%|██████▊   | 808/1187 [05:41<03:41,  1.71it/s, est. speed input: 167.24 toks/s, output: 4810.88 toks/s]Processed prompts:  78%|███████▊  | 931/1187 [05:42<01:01,  4.17it/s, est. speed input: 190.05 toks/s, output: 5529.28 toks/s]Processed prompts:  86%|████████▋ | 1025/1187 [05:46<00:26,  6.23it/s, est. speed input: 206.26 toks/s, output: 6025.94 toks/s]Processed prompts:  87%|████████▋ | 1030/1187 [05:59<00:25,  6.23it/s, est. speed input: 201.94 toks/s, output: 5903.35 toks/s]Processed prompts:  87%|████████▋ | 1031/1187 [06:41<01:13,  2.12it/s, est. speed input: 179.12 toks/s, output: 5237.53 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:41<00:00,  2.12it/s, est. speed input: 206.02 toks/s, output: 6033.03 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:41<00:00,  2.96it/s, est. speed input: 206.02 toks/s, output: 6033.03 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 66%|██████▋   | 788/1187 [00:00<00:00, 7874.34it/s]100%|██████████| 1187/1187 [00:00<00:00, 7811.01it/s]
Model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-220-merged, Accuracy: 0.42712721145745575
Result saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-220-merged_acc.txt
✅ Evaluation for checkpoint-220 completed successfully
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged
Result will be saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-327-merged_acc.txt
INFO 10-22 17:30:24 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:30:29 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged'}
INFO 10-22 17:30:46 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:30:46 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:30:46 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:30:46 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:47 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:30:51.432671318 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:51 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=848853)[0;0m WARNING 10-22 17:30:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:51 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged...
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:51 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:30:52 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=848853)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=848853)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:45<00:45, 45.08s/it]
[1;36m(EngineCore_DP0 pid=848853)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 26.18s/it]
[1;36m(EngineCore_DP0 pid=848853)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 29.04s/it]
[1;36m(EngineCore_DP0 pid=848853)[0;0m 
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:31:50 [default_loader.py:268] Loading weights took 58.12 seconds
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:31:50 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 58.423886 seconds
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:31:58 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/b88df4f7f6/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:31:58 [backends.py:550] Dynamo bytecode transform time: 7.23 s
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:31:58 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:04 [backends.py:215] Compiling a graph for dynamic shape takes 5.44 s
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:05 [monitor.py:34] torch.compile takes 12.67 s in total
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:06 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:06 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:06 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=848853)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 36.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:01, 31.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:01, 34.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 16/67 [00:00<00:01, 35.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:00<00:01, 37.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 26/67 [00:00<00:01, 39.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:00<00:00, 39.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 34/67 [00:00<00:00, 38.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:01<00:00, 37.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 38.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:01<00:00, 39.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:01<00:00, 39.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 57/67 [00:01<00:00, 39.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:01<00:00, 38.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 65/67 [00:01<00:00, 38.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 37.30it/s]
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:10 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:10 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=848853)[0;0m INFO 10-22 17:32:10 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.27 seconds
INFO 10-22 17:32:10 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:32:10 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  32%|███▏      | 377/1187 [00:00<00:00, 3766.61it/s]Adding requests:  64%|██████▍   | 759/1187 [00:00<00:00, 3790.73it/s]Adding requests:  97%|█████████▋| 1154/1187 [00:00<00:00, 3861.57it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3837.23it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [01:10<23:15:51, 70.62s/it, est. speed input: 0.62 toks/s, output: 29.00 toks/s]Processed prompts:   3%|▎         | 33/1187 [01:11<29:18,  1.52s/it, est. speed input: 30.76 toks/s, output: 951.84 toks/s]Processed prompts:  13%|█▎        | 158/1187 [01:12<04:11,  4.10it/s, est. speed input: 155.86 toks/s, output: 4479.89 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:21<02:37,  5.89it/s, est. speed input: 218.05 toks/s, output: 6447.38 toks/s]Processed prompts:  22%|██▏       | 258/1187 [01:39<02:37,  5.89it/s, est. speed input: 215.63 toks/s, output: 6352.23 toks/s]Processed prompts:  22%|██▏       | 259/1187 [02:32<10:11,  1.52it/s, est. speed input: 117.65 toks/s, output: 3464.36 toks/s]Processed prompts:  25%|██▍       | 291/1187 [02:32<07:30,  1.99it/s, est. speed input: 132.27 toks/s, output: 3884.75 toks/s]Processed prompts:  35%|███▌      | 416/1187 [02:33<02:50,  4.53it/s, est. speed input: 186.36 toks/s, output: 5516.17 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:41<01:50,  6.09it/s, est. speed input: 219.76 toks/s, output: 6485.38 toks/s]Processed prompts:  43%|████▎     | 516/1187 [02:59<01:50,  6.09it/s, est. speed input: 213.80 toks/s, output: 6307.45 toks/s]Processed prompts:  44%|████▎     | 517/1187 [03:56<06:30,  1.71it/s, est. speed input: 150.59 toks/s, output: 4443.39 toks/s]Processed prompts:  46%|████▋     | 549/1187 [03:57<04:59,  2.13it/s, est. speed input: 159.30 toks/s, output: 4706.09 toks/s]Processed prompts:  57%|█████▋    | 674/1187 [03:58<01:54,  4.47it/s, est. speed input: 196.47 toks/s, output: 5759.10 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:06<01:11,  5.83it/s, est. speed input: 218.50 toks/s, output: 6370.83 toks/s]Processed prompts:  65%|██████▌   | 773/1187 [04:19<01:10,  5.83it/s, est. speed input: 213.12 toks/s, output: 6202.49 toks/s]Processed prompts:  65%|██████▌   | 774/1187 [05:25<04:04,  1.69it/s, est. speed input: 166.55 toks/s, output: 4846.15 toks/s]Processed prompts:  68%|██████▊   | 806/1187 [05:25<03:02,  2.09it/s, est. speed input: 175.17 toks/s, output: 5040.22 toks/s]Processed prompts:  78%|███████▊  | 930/1187 [05:26<01:00,  4.28it/s, est. speed input: 199.19 toks/s, output: 5798.94 toks/s]Processed prompts:  86%|████████▋ | 1025/1187 [05:31<00:26,  6.01it/s, est. speed input: 215.62 toks/s, output: 6304.68 toks/s]Processed prompts:  87%|████████▋ | 1030/1187 [05:49<00:26,  6.01it/s, est. speed input: 213.21 toks/s, output: 6232.35 toks/s]Processed prompts:  87%|████████▋ | 1031/1187 [06:26<01:10,  2.22it/s, est. speed input: 186.07 toks/s, output: 5439.27 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:26<00:00,  2.22it/s, est. speed input: 213.96 toks/s, output: 6265.53 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:26<00:00,  3.07it/s, est. speed input: 213.96 toks/s, output: 6265.53 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 69%|██████▊   | 815/1187 [00:00<00:00, 8144.10it/s]100%|██████████| 1187/1187 [00:00<00:00, 8020.08it/s]
Model: /root/nlpexp/outputs/lora_rank16_lr8e-5/checkpoint-327-merged, Accuracy: 0.4102780117944398
Result saved to: ./eval_results/lora_rank16_lr8e-5/checkpoint-327-merged_acc.txt
✅ Evaluation for checkpoint-327 completed successfully
========================================
Start evaluating experiment: lora_rank32_lr5e-5
Model base dir: /root/nlpexp/outputs/lora_rank32_lr5e-5
Results will be saved under: ./eval_results/lora_rank32_lr5e-5
========================================
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged
Result will be saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-110-merged_acc.txt
INFO 10-22 17:39:08 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:39:12 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged'}
INFO 10-22 17:39:28 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:39:28 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:39:28 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:39:30 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:30 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:30 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:39:34.595350579 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:34 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=862985)[0;0m WARNING 10-22 17:39:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:34 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged...
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:35 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:39:35 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=862985)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=862985)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:46<00:46, 46.77s/it]
[1;36m(EngineCore_DP0 pid=862985)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 25.91s/it]
[1;36m(EngineCore_DP0 pid=862985)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 29.04s/it]
[1;36m(EngineCore_DP0 pid=862985)[0;0m 
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:33 [default_loader.py:268] Loading weights took 58.21 seconds
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:34 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 58.488853 seconds
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:44 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/a4dfbc58fa/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:44 [backends.py:550] Dynamo bytecode transform time: 9.58 s
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:44 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:50 [backends.py:215] Compiling a graph for dynamic shape takes 5.71 s
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:51 [monitor.py:34] torch.compile takes 15.29 s in total
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:52 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:52 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:52 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=862985)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 35.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:01, 33.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:01, 34.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 16/67 [00:00<00:01, 36.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|██▉       | 20/67 [00:00<00:01, 37.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 24/67 [00:00<00:01, 37.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:00<00:01, 38.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:00<00:00, 39.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:01<00:00, 39.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 40.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:01<00:00, 40.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:01<00:00, 40.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:01<00:00, 40.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:01<00:00, 38.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 37.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 38.17it/s]
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:56 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:56 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=862985)[0;0m INFO 10-22 17:40:56 [core.py:218] init engine (profile, create kv cache, warmup model) took 22.07 seconds
INFO 10-22 17:40:56 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:40:56 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  32%|███▏      | 376/1187 [00:00<00:00, 3703.84it/s]Adding requests:  63%|██████▎   | 751/1187 [00:00<00:00, 3729.01it/s]Adding requests:  96%|█████████▌| 1135/1187 [00:00<00:00, 3776.99it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3756.37it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [00:01<37:11,  1.88s/it, est. speed input: 27.64 toks/s, output: 26.58 toks/s]Processed prompts:   0%|          | 2/1187 [00:03<30:08,  1.53s/it, est. speed input: 42.41 toks/s, output: 54.12 toks/s]Processed prompts:   0%|          | 3/1187 [00:03<19:45,  1.00s/it, est. speed input: 57.13 toks/s, output: 87.96 toks/s]Processed prompts:   0%|          | 4/1187 [00:03<14:03,  1.40it/s, est. speed input: 67.53 toks/s, output: 122.18 toks/s]Processed prompts:   0%|          | 5/1187 [00:03<10:16,  1.92it/s, est. speed input: 81.99 toks/s, output: 156.96 toks/s]Processed prompts:   1%|          | 6/1187 [00:04<11:48,  1.67it/s, est. speed input: 85.43 toks/s, output: 172.96 toks/s]Processed prompts:   1%|          | 7/1187 [01:14<7:35:32, 23.16s/it, est. speed input: 6.04 toks/s, output: 38.57 toks/s]Processed prompts:   3%|▎         | 32/1187 [01:14<35:25,  1.84s/it, est. speed input: 28.34 toks/s, output: 723.74 toks/s]Processed prompts:  13%|█▎        | 153/1187 [01:15<04:27,  3.86it/s, est. speed input: 144.17 toks/s, output: 3972.50 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:19<02:11,  7.06it/s, est. speed input: 223.70 toks/s, output: 6499.17 toks/s]Processed prompts:  22%|██▏       | 262/1187 [01:23<02:35,  5.95it/s, est. speed input: 215.85 toks/s, output: 6221.26 toks/s]Processed prompts:  22%|██▏       | 265/1187 [01:25<02:45,  5.58it/s, est. speed input: 214.99 toks/s, output: 6148.79 toks/s]Processed prompts:  23%|██▎       | 268/1187 [01:31<03:56,  3.89it/s, est. speed input: 201.50 toks/s, output: 5722.60 toks/s]Processed prompts:  23%|██▎       | 268/1187 [01:49<03:56,  3.89it/s, est. speed input: 201.50 toks/s, output: 5722.60 toks/s]Processed prompts:  23%|██▎       | 269/1187 [02:44<24:14,  1.58s/it, est. speed input: 112.74 toks/s, output: 3203.27 toks/s]Processed prompts:  25%|██▍       | 293/1187 [02:45<14:36,  1.02it/s, est. speed input: 122.70 toks/s, output: 3493.64 toks/s]Processed prompts:  35%|███▍      | 413/1187 [02:46<03:29,  3.70it/s, est. speed input: 170.74 toks/s, output: 4937.89 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:49<01:47,  6.28it/s, est. speed input: 208.26 toks/s, output: 6048.73 toks/s]Processed prompts:  44%|████▎     | 518/1187 [02:54<02:03,  5.43it/s, est. speed input: 205.08 toks/s, output: 5934.87 toks/s]Processed prompts:  44%|████▍     | 521/1187 [02:55<02:07,  5.24it/s, est. speed input: 204.46 toks/s, output: 5926.38 toks/s]Processed prompts:  44%|████▍     | 524/1187 [02:57<02:16,  4.87it/s, est. speed input: 203.56 toks/s, output: 5881.60 toks/s]Processed prompts:  44%|████▍     | 526/1187 [02:57<02:13,  4.94it/s, est. speed input: 205.37 toks/s, output: 5886.63 toks/s]Processed prompts:  44%|████▍     | 528/1187 [03:03<03:31,  3.11it/s, est. speed input: 200.04 toks/s, output: 5730.77 toks/s]Processed prompts:  45%|████▍     | 529/1187 [04:17<29:26,  2.69s/it, est. speed input: 142.79 toks/s, output: 4090.27 toks/s]Processed prompts:  47%|████▋     | 553/1187 [04:18<13:40,  1.29s/it, est. speed input: 148.58 toks/s, output: 4269.76 toks/s]Processed prompts:  57%|█████▋    | 672/1187 [04:19<02:25,  3.55it/s, est. speed input: 181.01 toks/s, output: 5191.97 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:22<01:07,  6.19it/s, est. speed input: 205.07 toks/s, output: 5883.55 toks/s]Processed prompts:  65%|██████▌   | 774/1187 [04:25<01:14,  5.57it/s, est. speed input: 203.63 toks/s, output: 5831.02 toks/s]Processed prompts:  65%|██████▌   | 777/1187 [04:27<01:16,  5.33it/s, est. speed input: 203.48 toks/s, output: 5823.79 toks/s]Processed prompts:  66%|██████▌   | 780/1187 [04:27<01:16,  5.34it/s, est. speed input: 203.94 toks/s, output: 5828.13 toks/s]Processed prompts:  66%|██████▌   | 782/1187 [04:28<01:18,  5.14it/s, est. speed input: 203.78 toks/s, output: 5819.58 toks/s]Processed prompts:  66%|██████▌   | 784/1187 [04:29<01:21,  4.96it/s, est. speed input: 203.74 toks/s, output: 5819.87 toks/s]Processed prompts:  66%|██████▌   | 786/1187 [04:30<01:29,  4.46it/s, est. speed input: 203.21 toks/s, output: 5804.53 toks/s]Processed prompts:  66%|██████▋   | 787/1187 [04:30<01:29,  4.49it/s, est. speed input: 203.26 toks/s, output: 5808.08 toks/s]Processed prompts:  66%|██████▋   | 789/1187 [04:30<01:27,  4.57it/s, est. speed input: 204.40 toks/s, output: 5808.42 toks/s]Processed prompts:  67%|██████▋   | 790/1187 [04:32<02:04,  3.19it/s, est. speed input: 203.50 toks/s, output: 5777.62 toks/s]Processed prompts:  67%|██████▋   | 791/1187 [04:33<02:52,  2.30it/s, est. speed input: 202.59 toks/s, output: 5745.25 toks/s]Processed prompts:  67%|██████▋   | 792/1187 [04:34<03:07,  2.11it/s, est. speed input: 202.37 toks/s, output: 5736.94 toks/s]Processed prompts:  67%|██████▋   | 793/1187 [04:35<03:57,  1.66it/s, est. speed input: 201.60 toks/s, output: 5716.33 toks/s]Processed prompts:  67%|██████▋   | 794/1187 [04:56<26:22,  4.03s/it, est. speed input: 187.96 toks/s, output: 5326.68 toks/s]Processed prompts:  67%|██████▋   | 795/1187 [05:49<1:29:36, 13.71s/it, est. speed input: 159.64 toks/s, output: 4526.64 toks/s]Processed prompts:  69%|██████▉   | 819/1187 [05:49<10:52,  1.77s/it, est. speed input: 166.22 toks/s, output: 4661.65 toks/s]  Processed prompts:  79%|███████▊  | 933/1187 [05:50<01:07,  3.78it/s, est. speed input: 186.71 toks/s, output: 5312.94 toks/s]Processed prompts:  86%|████████▋ | 1025/1187 [05:51<00:22,  7.14it/s, est. speed input: 203.13 toks/s, output: 5828.76 toks/s]Processed prompts:  87%|████████▋ | 1031/1187 [05:54<00:23,  6.63it/s, est. speed input: 202.83 toks/s, output: 5825.76 toks/s]Processed prompts:  87%|████████▋ | 1035/1187 [05:55<00:23,  6.48it/s, est. speed input: 203.04 toks/s, output: 5833.41 toks/s]Processed prompts:  87%|████████▋ | 1038/1187 [05:55<00:23,  6.48it/s, est. speed input: 203.30 toks/s, output: 5837.63 toks/s]Processed prompts:  88%|████████▊ | 1041/1187 [05:56<00:22,  6.51it/s, est. speed input: 203.56 toks/s, output: 5847.97 toks/s]Processed prompts:  88%|████████▊ | 1043/1187 [05:56<00:23,  6.18it/s, est. speed input: 203.61 toks/s, output: 5849.16 toks/s]Processed prompts:  88%|████████▊ | 1045/1187 [05:56<00:22,  6.45it/s, est. speed input: 204.12 toks/s, output: 5858.62 toks/s]Processed prompts:  88%|████████▊ | 1047/1187 [05:57<00:25,  5.39it/s, est. speed input: 203.85 toks/s, output: 5853.57 toks/s]Processed prompts:  88%|████████▊ | 1049/1187 [05:59<00:32,  4.18it/s, est. speed input: 203.49 toks/s, output: 5843.28 toks/s]Processed prompts:  88%|████████▊ | 1050/1187 [05:59<00:33,  4.15it/s, est. speed input: 203.70 toks/s, output: 5839.86 toks/s]Processed prompts:  89%|████████▊ | 1051/1187 [05:59<00:35,  3.80it/s, est. speed input: 203.56 toks/s, output: 5837.91 toks/s]Processed prompts:  89%|████████▊ | 1052/1187 [06:11<03:42,  1.65s/it, est. speed input: 197.36 toks/s, output: 5657.49 toks/s]Processed prompts:  89%|████████▊ | 1053/1187 [06:35<10:54,  4.89s/it, est. speed input: 185.74 toks/s, output: 5321.63 toks/s]Processed prompts:  89%|████████▉ | 1054/1187 [06:43<12:01,  5.43s/it, est. speed input: 182.18 toks/s, output: 5221.11 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:43<00:00,  5.43s/it, est. speed input: 204.79 toks/s, output: 5895.34 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:43<00:00,  2.94it/s, est. speed input: 204.79 toks/s, output: 5895.34 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 68%|██████▊   | 805/1187 [00:00<00:00, 8045.67it/s]100%|██████████| 1187/1187 [00:00<00:00, 8094.90it/s]
Model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-110-merged, Accuracy: 0.43049705139005895
Result saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-110-merged_acc.txt
✅ Evaluation for checkpoint-110 completed successfully
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged
Result will be saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-220-merged_acc.txt
INFO 10-22 17:48:07 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:48:12 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged'}
INFO 10-22 17:48:28 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:48:28 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:48:28 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:48:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:30 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:30 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:48:34.247531158 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:34 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=877601)[0;0m WARNING 10-22 17:48:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:34 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged...
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:34 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:48:34 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=877601)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=877601)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:45<00:45, 45.90s/it]
[1;36m(EngineCore_DP0 pid=877601)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 26.63s/it]
[1;36m(EngineCore_DP0 pid=877601)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:59<00:00, 29.52s/it]
[1;36m(EngineCore_DP0 pid=877601)[0;0m 
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:34 [default_loader.py:268] Loading weights took 59.10 seconds
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:34 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 59.380828 seconds
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:47 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/333e649166/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:47 [backends.py:550] Dynamo bytecode transform time: 12.22 s
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:47 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:53 [backends.py:215] Compiling a graph for dynamic shape takes 5.84 s
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:54 [monitor.py:34] torch.compile takes 18.05 s in total
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:55 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:55 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:55 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=877601)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 34.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:02, 29.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:00<00:01, 34.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:00<00:01, 37.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:00<00:01, 38.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:00<00:00, 39.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:00<00:00, 40.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:00<00:00, 41.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 40.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:01<00:00, 39.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:01<00:00, 38.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 57/67 [00:01<00:00, 39.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:01<00:00, 38.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▊| 66/67 [00:01<00:00, 39.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 38.30it/s]
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:58 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:58 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=877601)[0;0m INFO 10-22 17:49:58 [core.py:218] init engine (profile, create kv cache, warmup model) took 24.10 seconds
INFO 10-22 17:49:59 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:49:59 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  28%|██▊       | 337/1187 [00:00<00:00, 3365.94it/s]Adding requests:  57%|█████▋    | 674/1187 [00:00<00:00, 3340.60it/s]Adding requests:  86%|████████▋ | 1024/1187 [00:00<00:00, 3410.84it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3385.64it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [00:03<1:18:07,  3.95s/it, est. speed input: 10.37 toks/s, output: 42.76 toks/s]Processed prompts:   0%|          | 2/1187 [00:05<50:43,  2.57s/it, est. speed input: 18.37 toks/s, output: 71.33 toks/s]  Processed prompts:   0%|          | 3/1187 [00:32<4:30:21, 13.70s/it, est. speed input: 5.20 toks/s, output: 43.75 toks/s]Processed prompts:   0%|          | 4/1187 [01:12<7:53:54, 24.04s/it, est. speed input: 2.94 toks/s, output: 47.94 toks/s]Processed prompts:   2%|▏         | 24/1187 [01:12<38:14,  1.97s/it, est. speed input: 22.64 toks/s, output: 611.04 toks/s]Processed prompts:  10%|▉         | 118/1187 [01:13<05:01,  3.55it/s, est. speed input: 113.91 toks/s, output: 3226.89 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:21<02:09,  7.18it/s, est. speed input: 216.44 toks/s, output: 6370.54 toks/s]Processed prompts:  22%|██▏       | 258/1187 [01:39<02:09,  7.18it/s, est. speed input: 211.40 toks/s, output: 6219.18 toks/s]Processed prompts:  22%|██▏       | 259/1187 [01:57<05:44,  2.69it/s, est. speed input: 151.69 toks/s, output: 4469.63 toks/s]Processed prompts:  22%|██▏       | 260/1187 [02:38<11:34,  1.33it/s, est. speed input: 112.78 toks/s, output: 3322.74 toks/s]Processed prompts:  22%|██▏       | 261/1187 [02:38<11:26,  1.35it/s, est. speed input: 113.42 toks/s, output: 3332.95 toks/s]Processed prompts:  24%|██▎       | 280/1187 [02:39<08:19,  1.82it/s, est. speed input: 121.18 toks/s, output: 3572.34 toks/s]Processed prompts:  32%|███▏      | 374/1187 [02:40<02:43,  4.98it/s, est. speed input: 156.82 toks/s, output: 4748.31 toks/s]Processed prompts:  42%|████▏     | 501/1187 [02:41<01:04, 10.58it/s, est. speed input: 214.33 toks/s, output: 6322.82 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:48<01:27,  7.74it/s, est. speed input: 209.95 toks/s, output: 6202.76 toks/s]Processed prompts:  44%|████▎     | 517/1187 [02:59<01:26,  7.74it/s, est. speed input: 206.95 toks/s, output: 6077.01 toks/s]Processed prompts:  44%|████▎     | 518/1187 [03:23<04:26,  2.51it/s, est. speed input: 176.19 toks/s, output: 5174.81 toks/s]Processed prompts:  44%|████▎     | 519/1187 [04:04<09:31,  1.17it/s, est. speed input: 147.46 toks/s, output: 4314.09 toks/s]Processed prompts:  45%|████▌     | 539/1187 [04:04<07:04,  1.53it/s, est. speed input: 152.32 toks/s, output: 4470.21 toks/s]Processed prompts:  53%|█████▎    | 632/1187 [04:05<02:21,  3.91it/s, est. speed input: 179.71 toks/s, output: 5231.31 toks/s]Processed prompts:  64%|██████▍   | 757/1187 [04:06<00:52,  8.22it/s, est. speed input: 215.13 toks/s, output: 6246.82 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:12<01:00,  6.88it/s, est. speed input: 213.22 toks/s, output: 6199.06 toks/s]Processed prompts:  65%|██████▌   | 775/1187 [04:17<01:15,  5.48it/s, est. speed input: 210.41 toks/s, output: 6104.77 toks/s]Processed prompts:  65%|██████▌   | 776/1187 [04:29<01:15,  5.48it/s, est. speed input: 203.42 toks/s, output: 5895.70 toks/s]Processed prompts:  65%|██████▌   | 777/1187 [04:29<02:06,  3.23it/s, est. speed input: 201.67 toks/s, output: 5838.30 toks/s]Processed prompts:  66%|██████▌   | 778/1187 [04:49<04:07,  1.66it/s, est. speed input: 188.34 toks/s, output: 5453.83 toks/s]Processed prompts:  66%|██████▌   | 779/1187 [05:32<10:20,  1.52s/it, est. speed input: 164.14 toks/s, output: 4752.02 toks/s]Processed prompts:  67%|██████▋   | 799/1187 [05:32<06:07,  1.06it/s, est. speed input: 170.45 toks/s, output: 4870.36 toks/s]Processed prompts:  75%|███████▌  | 892/1187 [05:33<01:21,  3.64it/s, est. speed input: 188.02 toks/s, output: 5428.85 toks/s]Processed prompts:  85%|████████▌ | 1014/1187 [05:33<00:20,  8.37it/s, est. speed input: 211.97 toks/s, output: 6169.79 toks/s]Processed prompts:  87%|████████▋ | 1027/1187 [05:38<00:22,  7.15it/s, est. speed input: 211.37 toks/s, output: 6159.81 toks/s]Processed prompts:  87%|████████▋ | 1034/1187 [05:49<00:21,  7.15it/s, est. speed input: 206.41 toks/s, output: 6013.57 toks/s]Processed prompts:  87%|████████▋ | 1035/1187 [06:02<00:49,  3.10it/s, est. speed input: 199.21 toks/s, output: 5803.98 toks/s]Processed prompts:  87%|████████▋ | 1036/1187 [06:31<01:39,  1.51it/s, est. speed input: 184.49 toks/s, output: 5374.36 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:31<00:00,  1.51it/s, est. speed input: 211.16 toks/s, output: 6163.34 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:31<00:00,  3.03it/s, est. speed input: 211.16 toks/s, output: 6163.34 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 70%|██████▉   | 828/1187 [00:00<00:00, 8277.94it/s]100%|██████████| 1187/1187 [00:00<00:00, 8272.75it/s]
Model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-220-merged, Accuracy: 0.43133951137320975
Result saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-220-merged_acc.txt
✅ Evaluation for checkpoint-220 completed successfully
----------------------------------------
Evaluating model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged
Result will be saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-327-merged_acc.txt
INFO 10-22 17:57:01 [__init__.py:216] Automatically detected platform cuda.
INFO 10-22 17:57:06 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged'}
INFO 10-22 17:57:23 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-22 17:57:23 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.
INFO 10-22 17:57:23 [__init__.py:1815] Using max model len 131072
INFO 10-22 17:57:25 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:25 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:25 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged', speculative_config=None, tokenizer='/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W1022 17:57:29.881845646 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:29 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=891661)[0;0m WARNING 10-22 17:57:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:30 [gpu_model_runner.py:2338] Starting to load model /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged...
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:30 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:57:30 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=891661)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=891661)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:48<00:48, 48.30s/it]
[1;36m(EngineCore_DP0 pid=891661)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 27.18s/it]
[1;36m(EngineCore_DP0 pid=891661)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.35s/it]
[1;36m(EngineCore_DP0 pid=891661)[0;0m 
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:31 [default_loader.py:268] Loading weights took 60.82 seconds
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:32 [gpu_model_runner.py:2392] Model loading took 2.9110 GiB and 61.101734 seconds
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:42 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/66406ae5ff/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:42 [backends.py:550] Dynamo bytecode transform time: 9.88 s
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:42 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:48 [backends.py:215] Compiling a graph for dynamic shape takes 5.68 s
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:49 [monitor.py:34] torch.compile takes 15.56 s in total
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:50 [gpu_worker.py:298] Available KV cache memory: 16.05 GiB
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:50 [kv_cache_utils.py:864] GPU KV cache size: 601,152 tokens
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:50 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 4.59x
[1;36m(EngineCore_DP0 pid=891661)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:01, 36.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:01, 36.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:00<00:01, 38.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:00<00:01, 39.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:00<00:01, 41.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:00<00:00, 41.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:00<00:00, 41.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:00<00:00, 41.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 41.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:01<00:00, 41.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:01<00:00, 41.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:01<00:00, 40.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:01<00:00, 39.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 37.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 39.90it/s]
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:53 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.47 GiB
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:53 [gpu_worker.py:391] Free memory on device (23.51/23.99 GiB) on startup. Desired GPU memory utilization is (0.85, 20.39 GiB). Actual usage is 2.91 GiB for weight, 1.41 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=16578150809` to fit into requested memory, or `--kv-cache-memory=19928246272` to fully utilize gpu memory. Current kv cache memory in use is 17236656537 bytes.
[1;36m(EngineCore_DP0 pid=891661)[0;0m INFO 10-22 17:58:53 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.80 seconds
INFO 10-22 17:58:54 [llm.py:295] Supported_tasks: ['generate']
INFO 10-22 17:58:54 [__init__.py:36] No IOProcessor plugins requested by the model
Dataset size: 1187
Starting inference for model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged...
Adding requests:   0%|          | 0/1187 [00:00<?, ?it/s]AMP: CUDA graph is not supported
Adding requests:  30%|██▉       | 351/1187 [00:00<00:00, 3506.99it/s]Adding requests:  61%|██████    | 722/1187 [00:00<00:00, 3621.99it/s]Adding requests:  91%|█████████▏| 1085/1187 [00:00<00:00, 3543.68it/s]Adding requests: 100%|██████████| 1187/1187 [00:00<00:00, 3537.62it/s]
Processed prompts:   0%|          | 0/1187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1187 [00:03<1:17:12,  3.91s/it, est. speed input: 10.50 toks/s, output: 43.27 toks/s]Processed prompts:   0%|          | 2/1187 [00:04<40:39,  2.06s/it, est. speed input: 19.27 toks/s, output: 78.13 toks/s]  Processed prompts:   0%|          | 3/1187 [00:06<34:40,  1.76s/it, est. speed input: 21.41 toks/s, output: 99.99 toks/s]Processed prompts:   0%|          | 4/1187 [00:06<24:09,  1.22s/it, est. speed input: 30.87 toks/s, output: 132.89 toks/s]Processed prompts:   0%|          | 5/1187 [00:10<41:12,  2.09s/it, est. speed input: 25.82 toks/s, output: 120.71 toks/s]Processed prompts:   1%|          | 6/1187 [01:20<8:15:24, 25.17s/it, est. speed input: 3.81 toks/s, output: 40.81 toks/s]Processed prompts:   2%|▏         | 23/1187 [01:20<53:04,  2.74s/it, est. speed input: 19.23 toks/s, output: 473.71 toks/s]Processed prompts:  12%|█▏        | 144/1187 [01:21<04:54,  3.55it/s, est. speed input: 127.40 toks/s, output: 3496.07 toks/s]Processed prompts:  22%|██▏       | 257/1187 [01:30<02:38,  5.88it/s, est. speed input: 195.61 toks/s, output: 5697.06 toks/s]Processed prompts:  22%|██▏       | 261/1187 [01:34<02:57,  5.23it/s, est. speed input: 190.49 toks/s, output: 5551.50 toks/s]Processed prompts:  22%|██▏       | 263/1187 [01:49<02:56,  5.23it/s, est. speed input: 182.28 toks/s, output: 5287.61 toks/s]Processed prompts:  22%|██▏       | 264/1187 [02:51<13:56,  1.10it/s, est. speed input: 106.14 toks/s, output: 3077.59 toks/s]Processed prompts:  24%|██▎       | 281/1187 [02:51<11:12,  1.35it/s, est. speed input: 112.43 toks/s, output: 3274.56 toks/s]Processed prompts:  34%|███▍      | 401/1187 [02:53<03:29,  3.74it/s, est. speed input: 159.87 toks/s, output: 4666.16 toks/s]Processed prompts:  43%|████▎     | 513/1187 [02:58<01:52,  6.01it/s, est. speed input: 198.03 toks/s, output: 5790.41 toks/s]Processed prompts:  44%|████▎     | 517/1187 [03:05<02:12,  5.05it/s, est. speed input: 193.47 toks/s, output: 5643.39 toks/s]Processed prompts:  44%|████▍     | 520/1187 [03:10<02:36,  4.26it/s, est. speed input: 189.20 toks/s, output: 5513.11 toks/s]Processed prompts:  44%|████▍     | 522/1187 [03:10<02:37,  4.22it/s, est. speed input: 189.16 toks/s, output: 5506.75 toks/s]Processed prompts:  44%|████▍     | 522/1187 [03:29<02:37,  4.22it/s, est. speed input: 189.16 toks/s, output: 5506.75 toks/s]Processed prompts:  44%|████▍     | 523/1187 [04:19<14:41,  1.33s/it, est. speed input: 139.10 toks/s, output: 4050.67 toks/s]Processed prompts:  45%|████▌     | 540/1187 [04:20<10:27,  1.03it/s, est. speed input: 143.12 toks/s, output: 4174.51 toks/s]Processed prompts:  56%|█████▌    | 659/1187 [04:21<02:18,  3.81it/s, est. speed input: 175.58 toks/s, output: 5090.41 toks/s]Processed prompts:  65%|██████▍   | 769/1187 [04:24<01:00,  6.91it/s, est. speed input: 203.55 toks/s, output: 5893.02 toks/s]Processed prompts:  65%|██████▌   | 774/1187 [04:30<01:14,  5.51it/s, est. speed input: 200.07 toks/s, output: 5779.25 toks/s]Processed prompts:  66%|██████▌   | 778/1187 [04:37<01:35,  4.28it/s, est. speed input: 196.05 toks/s, output: 5659.94 toks/s]Processed prompts:  66%|██████▌   | 781/1187 [04:48<02:25,  2.80it/s, est. speed input: 189.00 toks/s, output: 5456.95 toks/s]Processed prompts:  66%|██████▌   | 782/1187 [05:45<08:41,  1.29s/it, est. speed input: 158.31 toks/s, output: 4570.33 toks/s]Processed prompts:  67%|██████▋   | 799/1187 [05:45<05:58,  1.08it/s, est. speed input: 163.90 toks/s, output: 4666.98 toks/s]Processed prompts:  77%|███████▋  | 917/1187 [05:46<01:06,  4.05it/s, est. speed input: 185.87 toks/s, output: 5349.21 toks/s]Processed prompts:  86%|████████▋ | 1025/1187 [05:47<00:21,  7.64it/s, est. speed input: 205.47 toks/s, output: 5966.73 toks/s]Processed prompts:  87%|████████▋ | 1032/1187 [05:53<00:25,  6.17it/s, est. speed input: 203.18 toks/s, output: 5907.55 toks/s]Processed prompts:  87%|████████▋ | 1037/1187 [05:56<00:26,  5.60it/s, est. speed input: 202.51 toks/s, output: 5880.66 toks/s]Processed prompts:  88%|████████▊ | 1039/1187 [06:09<00:26,  5.60it/s, est. speed input: 199.17 toks/s, output: 5785.18 toks/s]Processed prompts:  88%|████████▊ | 1040/1187 [06:40<01:41,  1.45it/s, est. speed input: 180.73 toks/s, output: 5250.57 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:40<00:00,  1.45it/s, est. speed input: 206.31 toks/s, output: 6001.22 toks/s]Processed prompts: 100%|██████████| 1187/1187 [06:40<00:00,  2.96it/s, est. speed input: 206.31 toks/s, output: 6001.22 toks/s]
Inference completed. Calculating rewards...
  0%|          | 0/1187 [00:00<?, ?it/s] 68%|██████▊   | 811/1187 [00:00<00:00, 8098.63it/s]100%|██████████| 1187/1187 [00:00<00:00, 8131.43it/s]
Model: /root/nlpexp/outputs/lora_rank32_lr5e-5/checkpoint-327-merged, Accuracy: 0.41954507160909854
Result saved to: ./eval_results/lora_rank32_lr5e-5/checkpoint-327-merged_acc.txt
✅ Evaluation for checkpoint-327 completed successfully
========================================
All evaluations finished.
Results are organized under: ./eval_results
